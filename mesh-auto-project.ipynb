{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from ratelimiter import RateLimiter\n",
    "import pubmed_parser as pp\n",
    "import nltk\n",
    "import matplotlib\n",
    "Entrez.email = 'amanda.sawyer@nih.gov'\n",
    "api_key = '86d72be66a4381e2e22c704615cbb9620c08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposedTermsPath = 'mesh-proposed-terms.csv'\n",
    "proposedTerms = pd.read_csv(proposedTermsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Title and Title/Abstract Posting from PubMed\n",
    "**Method 1: Testing Efficiency by using apply() and RateLimiter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for title and title/abstract\n",
    "#RateLimiter limits to max of 10 calls per second using my NCBI key, per their requirements: https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/\n",
    "#Find information on RateLimiter here: https://pypi.org/project/ratelimiter/\n",
    "#Find information on Pandas apply() function here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n",
    "\n",
    "@RateLimiter(max_calls=10, period=1)\n",
    "def getPubMedTitleTotal (term):\n",
    "    termCounts = 0\n",
    "    \n",
    "    try:\n",
    "        searchstring = term+' [ti]'\n",
    "        handle = Entrez.esearch(db='pubmed', term=searchstring)\n",
    "        result = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        termCounts = result['Count']\n",
    "    except:\n",
    "        print(\"Error:\", searchstring)\n",
    "            \n",
    "    return termCounts\n",
    "\n",
    "@RateLimiter(max_calls=10, period=1)\n",
    "def getPubMedTIABTotal (term):\n",
    "    termCounts = 0\n",
    "    \n",
    "    try:\n",
    "        searchstring = term+' [tiab]'\n",
    "        handle = Entrez.esearch(db='pubmed', term=searchstring)\n",
    "        result = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        termCounts = result['Count']\n",
    "    except:\n",
    "        print(\"Error:\", searchstring)\n",
    "            \n",
    "    return termCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#use apply(), and Python magic time function\n",
    "proposedTerms['Title Count'] = proposedTerms['term'].apply(getPubMedTitleTotal)\n",
    "proposedTerms['Title Abstract Count'] = proposedTerms['term'].apply(getPubMedTIABTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposedTerms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2 - Original Method using iteration instead of apply.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPubMedtotal(searchstring):\n",
    "    thingtoreturn=0\n",
    "\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=searchstring, retmax = 1000)\n",
    "        result = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        thingtoreturn=result[\"Count\"]\n",
    "    except:\n",
    "        print(\"Error:\",searchstring)\n",
    "\n",
    "    return thingtoreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_XLSX = pd.read_csv(proposedTermsPath, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "List_XLSX['TitleSearchTotal']=0\n",
    "List_XLSX['TIABSearchTotal']=0\n",
    "\n",
    "for index,row in List_XLSX.iterrows():\n",
    "    List_XLSX.loc[index,'TitleSearchTotal']=getPubMedtotal(row['term'] + \"[ti]\")\n",
    "    if index and not index % 250:\n",
    "        print(index,\"records processed\")\n",
    "print(\"Done.\")\n",
    "\n",
    " \n",
    "for index,row in List_XLSX.iterrows():\n",
    "    List_XLSX.loc[index,'TIABSearchTotal']=getPubMedtotal(row['term'] + \"[tiab]\")\n",
    "    if index and not index % 100:\n",
    "        print(index,\"records processed\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_XLSX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PI and MN from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@RateLimiter(max_calls=10, period=1)\n",
    "def getPMIDs (term) :\n",
    "    handle_test = Entrez.esearch(db='pubmed', term=term+'[ti]', retmax = 1000)\n",
    "    result_test = Entrez.read(handle_test)\n",
    "    handle_test.close()\n",
    "    pmid = result_test['IdList']\n",
    "    return pmid\n",
    "\n",
    "@RateLimiter(max_calls=10, period=1)\n",
    "def returnPMRecords (ids) :\n",
    "    tester = []\n",
    "    rate_limiter = RateLimiter(max_calls=3, period=1)\n",
    "\n",
    "    for x in ids :\n",
    "        with rate_limiter:\n",
    "            tester.append(pp.parse_xml_web(x, save_xml=False))\n",
    "    return tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newterm = input(\"Enter search term:\")\n",
    "ids = getPMIDs(newterm)\n",
    "tester = returnPMRecords(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a df\n",
    "newTermDF = pd.DataFrame(tester)\n",
    "#split keywords on ;\n",
    "newTermDF['keywordssplit'] = newTermDF.apply(lambda row: row.keywords.split(';'), axis = 1)\n",
    "#each keyword gets its own row\n",
    "newTermDF = newTermDF.explode('keywordssplit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a df for frequency of terms\n",
    "frequency = pd.DataFrame()\n",
    "frequency['Term Frequency'] = newTermDF['keywordssplit'].value_counts()\n",
    "frequency['Percentage'] = round((frequency['Term Frequency'] / newTermDF['keywordssplit'].value_counts().sum()) * 100, 2)\n",
    "frequency = frequency.reset_index()\n",
    "frequency[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top15 = frequency[:14]\n",
    "total = top15['Term Frequency'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = top15['Term Frequency'] / total * 100\n",
    "sizes = sizes.tolist()\n",
    "names = top15['index']\n",
    "names = names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "labels = names\n",
    "sizes = sizes\n",
    "cs=cm.gist_rainbow(np.arange(40)/40.)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=cs, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "ax1.axis('equal')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(13, 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring NLTK for Frequency Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of top keywords you want to look at\n",
    "#https://stackoverflow.com/questions/40206249/count-of-most-popular-words-in-a-pandas-dataframe\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "top_N = 25\n",
    "\n",
    "newTermDF['keywordssplit'] = newTermDF['keywordssplit'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "txt = newTermDF.keywordssplit.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n",
    "\n",
    "print('All frequencies, without STOPWORDS:')\n",
    "print('=' * 60)\n",
    "rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)\n",
    "print('=' * 60)\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "rslt.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Searching for Other Related Terms with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df from title, abstract keywords. using data collected from 1,000 articles for pt. 2. example of \"Default Mode Network\"\n",
    "ldadf = pd.DataFrame(tester)\n",
    "ldadf = ldadf[['title', 'abstract', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "#space out keywords, remove ;\n",
    "ldadf['keywords'] = ldadf['keywords'].str.replace(\";\", \" \")\n",
    "\n",
    "#place everything in lowercase and remove punctuation\n",
    "ldadf = ldadf.apply(lambda x: x.astype(str).str.lower())\n",
    "def remove_punctuation(x):\n",
    "    try:\n",
    "        x = x.str.replace('[^\\w\\s]','') #might be worth exploring a str.translate option to save memory/time\n",
    "    except:pass\n",
    "    return x\n",
    "\n",
    "ldadf = ldadf.apply(remove_punctuation)\n",
    "\n",
    "#remove numbers from keywords\n",
    "ldadf['keywords'] = ldadf['keywords'].str.replace('\\d+', '  ')\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "ldadf['title'] = ldadf.title.apply(lemmatize_text)\n",
    "ldadf['abstract'] = ldadf.abstract.apply(lemmatize_text)\n",
    "ldadf['keywords'] = ldadf.keywords.apply(lemmatize_text)\n",
    "\n",
    "#remove stopwords\n",
    "ldadf['title'] = ldadf['title'].map(lambda x: [t for t in x if t not in stopwords])\n",
    "ldadf['abstract'] = ldadf['abstract'].map(lambda x: [t for t in x if t not in stopwords])\n",
    "ldadf['keywords'] = ldadf['keywords'].map(lambda x: [t for t in x if t not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(ldadf['title'])\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in ldadf['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=10, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldadf['title'][510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[510]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model_tfidf, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Attempt 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "#import pyLDAvis.gensim\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "\n",
    "# Import dataset\n",
    "p_df = pd.DataFrame(tester)\n",
    "# Create sample of 10,000 reviews\n",
    "# Convert to array\n",
    "docs = array(p_df['keywords'])\n",
    "\n",
    "# Define function for tokenize and lemmatizing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs\n",
    "\n",
    "# Perform function on our document\n",
    "docs = docs_preprocessor(docs)\n",
    "#Create Biagram & Trigram Models \n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "            \n",
    "#Remove rare & common tokens \n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.2)\n",
    "#Create dictionary and corpus required for Topic Modeling\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "chunksize = 500 \n",
    "passes = 20 \n",
    "iterations = 400\n",
    "eval_every = 1  \n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "# Print the Keyword in the 5 topics\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "chunksize = 500 \n",
    "passes = 80 \n",
    "iterations = 400\n",
    "eval_every = 1  \n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "# Print the Keyword in the 5 topics\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_topics = 5\n",
    "#n_top_words = 9\n",
    "#my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=120, minimum_probability=0)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
